{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bRgXP4S7nH_h"
   },
   "source": [
    "> # **Task n°3:**  Data Science Questions\n",
    "\n",
    "Answer these questions and explain the logic of your thinking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mf86u6SbqtNf"
   },
   "source": [
    "## **Question1:**\n",
    "Accuracy is often used to measure the performance of machine learning models. Do you think that accuracy is a good performance metric? What alternatives do you suggest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is used to mesure classification models's performance in case of a regression problem i would use RMSE, but still in classification accuracy is not always the go-to metrics because in the case of imbalanced data accuracy won't help i suggest using precision, recall and f1- score and trying ROC plot and mesuring AUC in case the models are close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dHDn9VLvhUdY"
   },
   "source": [
    "## **Answer:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m96Jhcn5qtqb"
   },
   "source": [
    "## **Question2:**\n",
    "What approaches can you use to deal with an imabalanced dataset and which one of them do you recommand?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHqBs8Pdhh6H"
   },
   "source": [
    "## **Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with imbalanced data I won't definitely use accuracy as metric i would use presicion recall and f1 score and ROC / AUC , I will also avoid instance based algorithms such as KNN since our instances are not balanced i would try other algorithms such as RandomForest or SVM or XGBoost and GridSearchCV also using K-fold CrossValidation is extremely usefull to correctely validate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XbUMwPmxqt0A"
   },
   "source": [
    "## **Question3:**\n",
    "Suppose you have a dataset  tennis players' names, their atp points and their atp ranking at different dates.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Date |\tPlayer |\tatp_points |\tatp_rank\n",
    "--- | --- | --- | --- \n",
    "16/01/2017 |\tFederer R.\t| 1980 |\t17 |\n",
    "18/01/2017 |\tQuerrey S. |\t1355\t|\t32\n",
    "27/02/2017\t|\tFederer R. |\t3260\t|\t10\n",
    "01/03/2017\t|\tMurray A.| \t11540\t|\t1\n",
    "13/03/2017\t|\tFederer R.|\t3305\t|\t10\n",
    "14/03/2017\t|\tNadal R. |\t4415\t|\t6\n",
    "\n",
    "\n",
    "However, you have a relatively important number of missing data for the feature atp_points. How would you deal with these missing values (do not delete any row or column)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5zC391Shjh7"
   },
   "source": [
    "## **Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we're not going to delete any rows or columns because that's a huge loss in data and information, a common approach is to fill the missing values with the median or mean but instead just looking at the dataset i can see a correlation beteen atp_points and atp_rank for example points 3305 andd 3260 both got rank 10 the lowest points 1355 got 32 rank the highest points 11540 got the best rank 1 => the lower the rank the higher the points => strong and negative correlation thus we can fill atp_points using atp_rank we can even create a regression model to predict the missing atp_points using the rank and perhaps other features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OReQ7cn4qt3o"
   },
   "source": [
    "## **Question 4:**\n",
    "We want to predict which Netflix users will renew their subscription next month. What data would you need? How would you proceed to make the predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aR7rvzUYhksz"
   },
   "source": [
    "## **Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we need is : Number of logins per month / Number of shows watched(or watching) / Mean number of minutes per login / The type of subscription he has(or had, meaning how much money he was willing to pay) / How many times did he renew his subscription / Did he renew this month(binary 0 or 1) / User's age / User's gender / User's country "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "An8m-6p1c6xR"
   },
   "source": [
    "# **Question 5:**\n",
    "\n",
    "What are the steps for data preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQ9VgF10hmXM"
   },
   "source": [
    "## **Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wouldn't use the word steps when it comes for data preprocessing because it's not a finite process and the process is not actually the same for different AI problems such as NLP or Image processing or Sound Processing however possible techniques for data preprocessing are :\n",
    "\n",
    "In case of a normal ML problem such as Classification or Regression :\n",
    "\n",
    "dealing with missing values : filling them with mean or median or finding a correlation between them and another feature dropping rows or columns is the last solution or if there is a lot of missing values over 60 % of the total dataset. \n",
    "\n",
    "data visualization : plotting usefull graphs (plots , histograms , scatterplots , pie plots) to further understand our data \n",
    "and get a good feel of it , plotting a correlation map , finding out outliers using boxplots ,using PCA for dimensionality reduction can help with the plotting also using K-means and other unsupervised learning algorithms can help in data understanding and feature extraction.\n",
    "\n",
    "Scaling : getting our features into an appropriate scale is important we can use different types of transforms to perform this MinMaxscaler for normalization , StandScaler for standardization and many others depending on our features.\n",
    "\n",
    "Encoding : changing categorical to numerical using a LabelEncoder if the variable is ordinal or using OneHotEncoding if the variable is nominal. \n",
    "\n",
    "Smoothing : this is basically getting rid of outliers or applying mathematical functions on our data to get a better representation of it functions such as Logarithms , moving average and different kernels and filters => this is very useful in reducing the noise and image processing.\n",
    "\n",
    "Feature engineering : creating new features for example getting longitude and latitude from location , changing datetime object to which day of the week / which day of the month and other techniques, this process depends a lot on data understanding and our data types \n",
    "\n",
    "\n",
    "In case of NLP : removing stopwords, tokenizing ,using stemming / lemmitazation, using wordclouds ...\n",
    "\n",
    "In case of Image processing : Scaling pixels , trying different colorspaces , trying different filtering kernels , using GANs , adjusting and transforming images for more data.\n",
    "\n",
    "\n",
    "\n",
    "And of course getting more data in all cases helps we can move on then to data integration."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Engineering test : Task n°3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
